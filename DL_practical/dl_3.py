# -*- coding: utf-8 -*-
"""DL_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VlMACVeEYqgFYgKPLZLGkm2fyWl1UaVb
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

# Load dataset and display few samples
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'truck', 'ship', 'horse']

plt.figure(figsize = (10, 10))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(X_train[i])
    plt.title(class_names[y_train[i][0]])
    plt.axis('off')
plt.show()

# pre-process
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
(X_train, X_test) = X_train/255.0, X_test/255.0

n_classes = 10
(y_train, y_test) = to_categorical(y_train, n_classes), to_categorical(y_test, n_classes)

# model
model = Sequential([
    Conv2D(50, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu', input_shape = (32, 32, 3)),
    Conv2D(75, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu'),
    MaxPooling2D(pool_size = (2, 2)),
    Dropout(0.25),

    Conv2D(125, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu'),
    MaxPooling2D(pool_size = (2, 2)),
    Dropout(0.25),

    Flatten(),

    Dense(500, activation = 'relu'),
    Dropout(0.4),
    Dense(250, activation = 'relu'),
    Dropout(0.3),

    Dense(10, activation = 'softmax')
])
model.summary()

# compile model
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# early - stopping and reducing LR
early_stop = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)
reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, min_lr = 0.001)

# train model
losses = model.fit(X_train, y_train,
                   validation_data = (X_test, y_test),
                   batch_size = 128,
                   epochs = 40,
                   callbacks = [early_stop, reduce_lr])

# evaluate
test_loss, test_acc = model.evaluate(X_test, y_test, verbose = 2)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc * 100:.2f}")

# plot
plt.figure(figsize = (12, 4))
plt.subplot(1, 2, 1)
plt.plot(losses.history['accuracy'], label = "Train Accuracy")
plt.plot(losses.history['val_accuracy'], label = "Validation Accuracy")
plt.title("Model accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(loc = 'lower right')

plt.subplot(1, 2, 2)
plt.plot(losses.history['loss'], label = "Train Loss")
plt.plot(losses.history['val_loss'], label = "Validation Loss")
plt.title("Model losses")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(loc = 'upper right')

plt.show()