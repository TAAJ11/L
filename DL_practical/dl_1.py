# -*- coding: utf-8 -*-
"""DL_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ISAcPn04gDZ3lJQfRHLkEadkGzx2zmoZ
"""

import numpy as np
import tensorflow as tf
import pandas as pd

df = pd.read_csv('winequality-red.csv')
df.head()

df.shape

train_df = df.sample(frac=0.8, random_state=40)
val_df = df.drop(train_df.index)

max_val = train_df.max(axis=0)
min_val = train_df.min(axis=0)
range = max_val - min_val

train_df = (train_df - min_val)/(range)
val_df = (val_df - min_val)/(range)

X_train = train_df.drop('quality', axis=1)
X_val = train_df.drop('quality', axis=1)
y_train = train_df['quality']
y_val = train_df['quality']

input_shape = [X_train.shape[1]]
input_shape

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=1)
])
model.summary()

model.compile(optimizer = 'adam', loss = 'mae')

losses = model.fit(X_train, y_train,
                   validation_data = (X_val, y_val),
                   batch_size = 256,
                   epochs = 20)

model.predict(X_val.iloc[0:3, :])
y_val.iloc[0:3]

loss_df = pd.DataFrame(losses.history)
loss_df.loc[:,['loss', 'val_loss']].plot()
*****************************************************Diffrent-refere-below-for-inbuild-dataset-************************************
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data
X_train = X_train / 255.0  # Normalize the pixel values
X_test = X_test / 255.0    # Normalize the pixel values
y_train = to_categorical(y_train, 10)  # One-hot encode the labels
y_test = to_categorical(y_test, 10)    # One-hot encode the labels

# Build the FNN model
model = Sequential([
    Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images into vectors
    Dense(128, activation='relu'),  # Hidden layer with 128 neurons
    Dense(64, activation='relu'),   # Hidden layer with 64 neurons
    Dense(10, activation='softmax') # Output layer for 10 classes
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Plot training history
plt.figure(figsize=(12, 6))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
